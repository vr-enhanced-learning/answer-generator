{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Downloading required libraries\n",
        "!pip install -q transformers datasets sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15785af6-77f3-4b23-f115-e28a53229b37",
        "id": "Z_5ZkcnSs7CN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting the cache content of the transformer namespace\n",
        "!rm -rf $HOME/.cache/huggingface/\n",
        "!mkdir $HOME/.cache/huggingface/"
      ],
      "metadata": {
        "id": "ZpHZ_ek9s7CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C926E2Q1s7CP"
      },
      "outputs": [],
      "source": [
        "# Load the required libraries\n",
        "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning function\n",
        "def fine_tune_t5(model_dir, num_iterations=5):\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "\n",
        "    # Set up optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Fine-tune model for num_iterations\n",
        "    for i in range(num_iterations):\n",
        "        # Get context and question from user\n",
        "        context = input(\"Enter context: \")\n",
        "        question = input(\"Enter question: \")\n",
        "        answer = input(\"Enter answer:\")\n",
        "\n",
        "        input_text = f'The answer for the question: \"{question}\" based on the context: \"{context}\"'\n",
        "        output_text = f'{answer}'\n",
        "\n",
        "        # Tokenize inputs\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        target_ids = tokenizer.encode(output_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
        "        loss = criterion(outputs.logits.view(-1, outputs.logits.shape[-1]), target_ids.view(-1))\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print loss after each iteration\n",
        "        print(f\"Iteration {i+1}: Loss={loss.item()}\")\n",
        "\n",
        "        # Save model and tokenizer\n",
        "        output_dir = os.path.join(os.path.dirname(model_dir), \"fine_tuned_models\", f\"iteration_{i+1}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        print(f\"Model and tokenizer saved after iteration {i+1}\")\n"
      ],
      "metadata": {
        "id": "sN3bk1pXueM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_t5(\"/content/fine_tuned_model\", 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d559763-e3de-4e8a-d5cd-fe50fd4e0bb2",
        "id": "dENUi9vUueM4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter context: There are mysteries to the universe we were never meant to solve. But who we are and why we are here are not among. Those answers, we carry inside.\n",
            "Enter question: Where do we carry answers?\n",
            "Enter answer:Inside\n",
            "Iteration 1: Loss=0.9629542827606201\n",
            "Model and tokenizer saved after iteration 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Model - Local\n",
        "# Set model to eval mode for inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get context from user\n",
        "    context = input(\"Enter context: \")\n",
        "    question = input(\"Enter question: \")\n",
        "    input_text = f'The answer for the question: \"{question}\" based on the context: \"{context}\"'\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Generate answer\n",
        "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print generated answer\n",
        "    print(f\"Generated answer: {question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fff9b6-3e45-4952-c540-a7d115b80174",
        "id": "t_OgXqm8ueM5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter context: ou currently have zero compute units available. Resources offered free of charge are not guaranteed. Purchase more units here.\n",
            "Enter question: How many compute units available?\n",
            "Generated answer: zero compute units available\n"
          ]
        }
      ]
    }
  ]
}